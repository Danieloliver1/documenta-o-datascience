{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f71c2b",
   "metadata": {},
   "source": [
    "Aqui é documentação para ser analista de dados \n",
    "\n",
    "vai ser dividida por etapas           \n",
    "1-Analise de dados\n",
    "\n",
    "usando \n",
    "pandas \n",
    "numpy\n",
    "\n",
    "2-gráficos \n",
    "matplotlib\n",
    "\n",
    "3-aprendizado de maquina\n",
    "das quais aprendizado vai ter varias etapas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para o pandas funfionar\n",
    "#pip install pandas\n",
    "#pip install numpy\n",
    "#pip install openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tabela = pd.read_csv(\"link do arquivo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver informaçãoes\n",
    "\n",
    "tabela.info()\n",
    "\n",
    "#exibir dados\n",
    "\n",
    "tabela.head()\n",
    "tabela.head(quantidade de dados a ser exibido)\n",
    "\n",
    "#contar a quantidade de linha da coluna\n",
    "tabela['nome da coluna'].count()\n",
    "\n",
    "#calcula a media da coluna \n",
    "tabela['nome da coluna'].mean()\n",
    "\n",
    "#soma a coluna\n",
    "tabela['nome da coluna'].sum()\n",
    "\n",
    "#para contar os valores de uma coluna\n",
    "tabela['nome da coluna'].value_counts()\n",
    "\n",
    "#para contar os valores de uma coluna\n",
    "#normalize=True é para mostrar em porcentagem\n",
    "tabela['nome da coluna'].value_counts(normalize=True)\n",
    "\n",
    "#para formatar alguma variavel\n",
    "tabela.map({:.1%}.format)\n",
    "\n",
    "#axis = 1 significa que ta pegando a coluna\n",
    "\n",
    "#axis = 0 significa que ta pegando a linha\n",
    "\n",
    "#obs: por padrão axis = 1\n",
    "\n",
    "#loc serve para pesquisar registros dentro de um dataframe baseado no seu índices de linha ou coluna.\n",
    "tabela.loc[linha,coluna] = variavel\n",
    "#variavel que vai ta com o valor para substituir \n",
    "\n",
    "\n",
    "# criando uma variavel com uma condição\n",
    "variavel = tabela[tabela['coluna'] == 'dados que ta dentro da coluna'\n",
    "                  \n",
    "#apagando uma coluna da tabela\n",
    "#inplace significa que a tabela vai ser atualizada e alterada \n",
    "tabela.drop(['Coluna'], axis = 1, inplace=True)\n",
    "                  \n",
    "# convertendo as colunas para o tipo float\n",
    "tabela['coluna'] = tabela['coluna1'].astype(float)\n",
    "  \n",
    "#substituindo os valores vazio por zero da tabela\n",
    "tabela.fillna(0, inplace=True)\n",
    "\n",
    "# convertendo as colunas para o tipo float mais rígido\n",
    "# np é do numpy\n",
    "tabela['coluna'] = tabela['coluna1'].astype(np.float64)\n",
    "                  \n",
    "# convertendo a coluna 'coluna1' para o tipo float usando pd.to_numeric()\n",
    "tabela['coluna'] = pd.to_numeric(tabela['coluna'], errors='coerce')\n",
    "                  \n",
    "#para criar uma coluna \n",
    "tabela[\"nome da coluna criada\"] = especificações\n",
    "                  \n",
    "# procurando o valor 5 em todas as células do DataFrame\n",
    "variavel = tabela.applymap(lambda x: x == 5).any().any()\n",
    "                  \n",
    "# substituir todas as vírgulas em todas as colunas\n",
    "tabela = tabela.applymap(lambda x: x.replace(',', '') if isinstance(x, str) else x)\n",
    "#pode ser virgula ou qualquer coisa\n",
    "                  \n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c269968",
   "metadata": {},
   "source": [
    "`pd.pivot_table()` é uma função do pacote pandas em Python que permite criar uma tabela dinâmica (ou pivot table) a partir de um DataFrame. \n",
    "\n",
    "Uma tabela dinâmica é uma forma de resumir e analisar grandes conjuntos de dados de maneira rápida e fácil. Ela organiza os dados em uma grade, onde cada coluna representa uma categoria de dados e cada linha representa um item específico relacionado a essas categorias. O valor em cada célula da grade é uma agregação dos dados relacionados a essa categoria e a esse item.\n",
    "\n",
    "A função `pd.pivot_table()` tem vários argumentos que permitem personalizar como a tabela dinâmica será criada. Aqui estão os principais argumentos:\n",
    "\n",
    "- `data`: o DataFrame original que será usado para criar a tabela dinâmica.\n",
    "- `values`: a coluna do DataFrame que será usada como valor para as células da tabela dinâmica.\n",
    "- `index`: a coluna ou conjunto de colunas do DataFrame que serão usadas como índice para as linhas da tabela dinâmica.\n",
    "- `columns`: a coluna ou conjunto de colunas do DataFrame que serão usadas como colunas para as colunas da tabela dinâmica.\n",
    "- `aggfunc`: a função de agregação que será usada para calcular o valor em cada célula da tabela dinâmica. Por padrão, a função de agregação é a média.\n",
    "\n",
    "Ao chamar a função `pd.pivot_table()`, você pode especificar um ou mais argumentos para personalizar como a tabela dinâmica será criada. Em seguida, a função retorna uma nova tabela dinâmica, que você pode salvar em uma nova variável ou usar diretamente em outras análises.\n",
    "\n",
    "Uma vez que a tabela dinâmica é criada, você pode usar as funções e métodos do pandas para explorar e visualizar os dados em diferentes formatos. Por exemplo, você pode criar gráficos, filtrar os dados para visualizar apenas as informações relevantes, ou exportar a tabela dinâmica para um arquivo CSV ou Excel para compartilhar com outras pessoas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d38103",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nova_tabela = pd.pivot_table(tabela, values='Valor de Venda', index='Bandeira', columns='colunas criadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#se por acaso quer procurar um valor mais não sabe onde ta na coluna aqui abaixo mostra em qual coluna o valor se encontra\n",
    "\n",
    "# procurando a string 'mno' em todas as células do DataFrame\n",
    "encontrado = filtrando.applymap(lambda x: 'valor a ser procurado' in str(x)).any().any()\n",
    "\n",
    "if encontrado:\n",
    "    # procurando a string 'mno' em todas as células do DataFrame\n",
    "    indices = tabela.applymap(lambda x: 'valor a ser procurado' in str(x))\n",
    "    linhas, colunas = indices.index[indices.any(axis=1)], indices.any().idxmax()\n",
    "\n",
    "    # obtendo as colunas correspondentes aos índices\n",
    "    colunas_encontradas = tabela.loc[:, colunas]\n",
    "\n",
    "    # exibindo o resultado\n",
    "    display(colunas)\n",
    "else:\n",
    "    print(\"String não encontrada no DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb8001",
   "metadata": {},
   "source": [
    "Para criar uma nova coluna \"target\" com base em uma coluna existente em um DataFrame do Pandas, você pode seguir os seguintes passos:\n",
    "\n",
    "Suponha que você tenha um DataFrame chamado df com uma coluna existente chamada \"coluna existente\" e deseja criar uma nova coluna \"target\" com base nos valores da coluna existente. Para isso, você pode fazer o seguinte:\n",
    "\n",
    "Use a função apply para aplicar uma função a cada valor da coluna existente e criar uma nova coluna com base nesses valores.\n",
    "\n",
    "A função que você usa deve retornar o valor desejado para a nova coluna \"target\". Por exemplo, se você deseja criar uma nova coluna com o valor \"Sim\" para cada linha em que a coluna existente é maior que 10 e \"Não\" caso contrário, você pode usar uma função lambda que verifica a condição e retorna \"Sim\" ou \"Não\" com base nela.\n",
    "\n",
    "Aqui está um exemplo de código que cria uma nova coluna \"target\" com base em uma coluna existente chamada \"coluna existente\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crie um DataFrame de exemplo\n",
    "df = pd.DataFrame({'coluna existente': [5, 10, 15, 20, 25]})\n",
    "\n",
    "# Crie uma nova coluna 'target' com base na coluna existente\n",
    "df['target'] = df['coluna existente'].apply(lambda x: 'Sim' if x > 10 else 'Não')\n",
    "\n",
    "# Exiba o DataFrame resultante\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A saída deste código será:\n",
    "    \n",
    "   coluna existente target\n",
    "0                 5    Não\n",
    "1                10    Não\n",
    "2                15    Sim\n",
    "3                20    Sim\n",
    "4                25    Sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f2dc6",
   "metadata": {},
   "source": [
    "Aqui vamos transformar o tipo de coluna str para float\n",
    "\n",
    "quando transfoma por padrão o pandas coloca em notação cientifica\n",
    "aquiabaixo ta o codigo antes de tranformar para float e não fique em notação cientifica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9053219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuração de formatação do pandas para números de ponto flutuante\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca219c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformando as colunas do DataFrame para tipo float\n",
    "\n",
    "#variavel pode qualquer nome que você quiser\n",
    "for variavel in tabela.columns:\n",
    "    tabela[varival] = pd.to_numeric(tabela[varival], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27b183",
   "metadata": {},
   "source": [
    "Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dda991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grafico\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#criando grafico de linha\n",
    "\n",
    "#figsize=(largura, altura)\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "x = tabela.index\n",
    "y = tabela[\"coluna com valores foramatado tipo float\"]\n",
    "\n",
    "#passandos os dados\n",
    "\n",
    "#tipo de grafico\n",
    "#plot-linha\n",
    "#bar barra\n",
    "#pie pizza\n",
    "\n",
    "ax.plot(x,y)\n",
    "\n",
    "#personalizando o grafico adicionando rotulos para eixo x e para eixo y e titulo\n",
    "ax.set_xlabel(\"nome que vai ficar na linha x\")\n",
    "ax.set_ylabel(\"nome que vai ficar na linha y\")\n",
    "ax.set_title(\"Titulo do grafico\")\n",
    "\n",
    "\n",
    "# Atribuindo uma legenda\n",
    "plt.plot(x, y, label = 'Uma legenda')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Adicionar os valores em cima das barras\n",
    "for i, valor in enumerate(tabela[\"coluna com valores foramatado tipo float\"]):\n",
    "    ax.annotate(f'{valor:.2f}', (i, valor), ha='center', va='bottom')\n",
    "    \n",
    "#exibir grafico\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#exemplos\n",
    "\n",
    "# Criando um gráfico de barra\n",
    "# x pode ser o index\n",
    "# y pode ser o valor \n",
    "# x e y pode ser uma coluna\n",
    "# x2 e y2 pode ser outra coluna para compara uma coluna com a outra\n",
    "# plt.bar(x, y, label = 'Barrar1', color = 'r')\n",
    "# plt.bar(x2, y2, label = 'Barras2', color = 'y')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Criando um gráfico de pizza\n",
    "# plt.pie(fatias, labels = atividades, colors = colunas, startangle = 90, shadow = True, explode = (0.1, 0, 0))\n",
    "# plt.show()\n",
    "\n",
    "# Criando um gráfico de pontos\n",
    "# o * pode ser qualquer coisa \n",
    "# plt.scatter(x, y, label = 'Pontos', color = 'b', marker = '*', s = 100)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Pontos do Gráfico na côr Preta (c = 'k')\n",
    "# plt.scatter(data1['Preço (R$)'], data1['Área (m2)'], c='k')\n",
    "# plt.xlabel(\"Preços em Real\")\n",
    "# plt.ylabel(\"Área em metros quadrados\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# valor = np.array([20,22,24,26,28,30,32])\n",
    "# freq = np.array([100, 150, 170,180, 160, 120, 90])\n",
    "# width_n = 1.75   # Largura das Colunas \n",
    "# bar_color = 'k'  # Côr da barra = Preto\n",
    "# plt.bar(valor, freq, width=width_n, color=bar_color)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# votos = np.array([842.201, 488.775, 553.424, 424.307, 272.500, 381.512, 261.386])\n",
    "# candi = ['Candidato A', 'Candidato B', 'Candidato C', 'Candidato D', 'Candidato E',\n",
    "#          'Candidato F', 'Candidato G']\n",
    "# cores=['gold', 'red', 'blue', 'magenta', 'green','lightskyblue', 'yellowgreen']\n",
    "# # o atributo explode indica que fatia do gráfico será destacada. No exemplo abaixo, será a primeira fatia. A quantidade de valores é igual ao número de fatias do gráfico. \n",
    "# explode = (0.1, 0, 0, 0, 0, 0, 0)  # explode 1st slice\n",
    "# # Atribuindo um título ao gráfico\n",
    "# plt.title('Eleição 2020 - Total de Votos')\n",
    "# plt.pie(votos, explode=explode, labels=candi, colors=cores, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "# #Adiciona Legenda\n",
    "# plt.legend(candi, bbox_to_anchor=(1.3, 1.3),loc='upper right')\n",
    "# #Centraliza o gráfico\n",
    "# plt.axis('equal')\n",
    "# #Ajusta o espaçamento para evitar o recorte do rótulo\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab2b45",
   "metadata": {},
   "source": [
    "Antes de realizar a etapa de treinamento e teste em um modelo de aprendizado de máquina usando o Pandas, existem algumas etapas importantes de pré-processamento de dados que você deve seguir. Algumas das etapas comuns incluem:\n",
    "\n",
    "1. Lidar com dados ausentes: Verifique se há dados ausentes em seu conjunto de dados e decida como você deseja lidar com eles. Você pode optar por preencher os valores ausentes com a média ou a mediana dos dados existentes ou simplesmente excluí-los do conjunto de dados.\n",
    "\n",
    "2. Lidar com dados categóricos: Se o seu conjunto de dados contém dados categóricos, como nomes de cores, você precisará convertê-los em valores numéricos para que o modelo possa processá-los. Você pode usar a codificação one-hot para fazer isso.\n",
    "\n",
    "3. Normalização de dados: Em alguns casos, pode ser necessário normalizar os dados para que eles estejam na mesma escala. Isso pode ser importante, por exemplo, quando os dados de entrada têm diferentes unidades ou escalas, ou quando o algoritmo de aprendizado de máquina requer dados normalizados.\n",
    "\n",
    "4. Separar dados de treinamento e teste: É importante dividir o conjunto de dados em conjuntos separados de treinamento e teste para que você possa avaliar a precisão do modelo. A regra geral é usar cerca de 80% dos dados para treinamento e 20% para teste.\n",
    "\n",
    "5. Lidar com valores discrepantes: Verifique se há valores discrepantes em seus dados e decida como deseja lidar com eles. Você pode optar por removê-los ou substituí-los por valores médios.\n",
    "\n",
    "Essas são algumas das etapas comuns de pré-processamento de dados que você pode seguir antes de realizar a etapa de treinamento e teste em um modelo de aprendizado de máquina usando o Pandas. Lembre-se de que o pré-processamento dos dados é uma etapa importante no processo de construção de um modelo de aprendizado de máquina preciso e eficaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d349fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pip install scikit-learn\n",
    "\n",
    "#para treinamento de dados aqui faz a separação de teste\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4badb5",
   "metadata": {},
   "source": [
    "O aprendizado de máquina (ou \"machine learning\") é uma área da ciência da computação que envolve o desenvolvimento de algoritmos e modelos que podem aprender a partir de dados e fazer previsões ou tomar decisões automaticamente. O Python é uma das principais linguagens de programação usadas para desenvolver aplicativos de aprendizado de máquina, e há muitas bibliotecas e ferramentas disponíveis para facilitar esse processo.\n",
    "\n",
    "O Scikit-learn é uma das bibliotecas mais populares do Python para aprendizado de máquina. Ele contém uma ampla variedade de algoritmos de aprendizado de máquina e ferramentas para processamento de dados, pré-processamento, seleção de recursos, validação cruzada e outras tarefas comuns em aprendizado de máquina.\n",
    "\n",
    "A classe sklearn.pipeline.Pipeline é uma ferramenta útil do Scikit-learn que permite criar um fluxo de trabalho (ou \"pipeline\") para processar dados de entrada antes de treinar um modelo. Ele permite que você encapsule várias etapas de pré-processamento em um único objeto e execute-as em uma sequência específica.\n",
    "\n",
    "O pipeline é construído com uma série de etapas que são fornecidas como uma lista de tuplas. Cada tupla contém o nome da etapa e o objeto correspondente. As etapas são executadas em sequência, com os dados de entrada sendo processados por cada etapa sucessiva antes de serem enviados para a próxima.\n",
    "\n",
    "Por exemplo, suponha que você queira criar um pipeline para treinar um modelo de regressão logística usando dados de entrada que contêm valores numéricos e categóricos. Você pode usar o pipeline para executar as seguintes etapas em sequência:\n",
    "\n",
    "Converter os valores categóricos em valores numéricos usando a codificação one-hot.\n",
    "Normalizar os valores numéricos para ter uma média de zero e uma variância unitária.\n",
    "Treinar o modelo de regressão logística.\n",
    "O código para criar esse pipeline seria semelhante ao seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30344de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pepiline é um preprocessamento de dados antes do aprendizado de maquinas\n",
    "\n",
    "#exemplo usando o pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Crie as etapas do pipeline\n",
    "categorical_encoder = OneHotEncoder()\n",
    "numerical_scaler = StandardScaler()\n",
    "logistic_reg = LogisticRegression()\n",
    "\n",
    "# Crie o pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('categorical_encoder', categorical_encoder),\n",
    "    ('numerical_scaler', numerical_scaler),\n",
    "    ('logistic_reg', logistic_reg)\n",
    "])\n",
    "\n",
    "\n",
    "# Com o pipeline criado, você pode treinar o modelo chamando o método fit do pipeline com os dados de entrada e saída:\n",
    "\n",
    "# Treinar o modelo\n",
    "pipeline.fit(X_treino, y_treino)\n",
    "\n",
    "# E, em seguida, você pode fazer previsões em novos dados chamando o método predict do pipeline:\n",
    "\n",
    "# Fazer previsões em novos dados\n",
    "y_pred = pipeline.predict(X_teste)\n",
    "\n",
    "#Em resumo, o sklearn.pipeline. Pipeline é uma ferramenta útil para simplificar o processo de pré-processamento \n",
    "#de dados em aprendizado de máquina e executar várias etapas de processamento de dados em uma sequência específica \n",
    "#antes do treinamento do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510b1bd",
   "metadata": {},
   "source": [
    "Todos os modelos de aprendizado de máquina podem usar o `Pipeline` do Scikit-learn. O objetivo do `Pipeline` é simplificar o processo de pré-processamento de dados e melhorar a reprodutibilidade dos resultados. \n",
    "\n",
    "Os modelos de aprendizado de máquina geralmente envolvem várias etapas de pré-processamento de dados, como a limpeza e a normalização dos dados, a seleção de recursos e a transformação dos dados de entrada em um formato adequado para o modelo. O `Pipeline` permite que você defina um fluxo de trabalho sequencial que encapsula essas etapas de pré-processamento em um único objeto. Isso pode tornar o código mais fácil de manter e ajudar a evitar erros de programação que podem ocorrer quando as etapas de pré-processamento são executadas separadamente.\n",
    "\n",
    "Alguns exemplos de modelos de aprendizado de máquina que podem se beneficiar do uso do `Pipeline` incluem:\n",
    "\n",
    "- Regressão linear e logística\n",
    "- Árvores de decisão\n",
    "- Redes neurais artificiais\n",
    "- Máquinas de vetores de suporte\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Classificação Naive Bayes\n",
    "- Clustering com k-means\n",
    "\n",
    "Em geral, o `Pipeline` pode ser usado com qualquer modelo de aprendizado de máquina que seja compatível com as ferramentas de pré-processamento do Scikit-learn. Isso inclui muitos dos modelos mais populares e amplamente usados no aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelos de pipeline\n",
    "\n",
    "# Canalização \n",
    "\n",
    "from sklearn.pipeline import pipeline.FeatureUnion   # Concatena os resultados de vários objetos transformadores.\n",
    "from sklearn.pipeline import pipeline.Pipeline       # Pipeline de transformações com um estimador final.\n",
    "from sklearn.pipeline import pipeline.make_pipeline  # Construa a Pipelinea partir dos estimadores dados.\n",
    "from sklearn.pipeline import pipeline.make_union     # Construa um FeatureUnion a partir dos transformadores fornecidos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4c68a",
   "metadata": {},
   "source": [
    "O módulo `sklearn.datasets` do Scikit-learn fornece funções para carregar conjuntos de dados já existentes para fins de treinamento e teste de algoritmos de aprendizado de máquina. Esses conjuntos de dados incluem exemplos de problemas de classificação, regressão e clustering, que podem ser usados para avaliar a precisão e a eficácia de diferentes algoritmos de aprendizado de máquina.\n",
    "\n",
    "O uso de conjuntos de dados padronizados ajuda a garantir a reprodutibilidade dos resultados e permite que pesquisadores e desenvolvedores comparem diferentes algoritmos de aprendizado de máquina em condições iguais. O módulo `sklearn.datasets` fornece uma variedade de conjuntos de dados populares, como o conjunto de dados iris, o conjunto de dados MNIST para reconhecimento de dígitos escritos à mão, o conjunto de dados Boston House Prices para regressão, entre outros.\n",
    "\n",
    "Além disso, o módulo `sklearn.datasets` também fornece funções para gerar dados sintéticos, o que pode ser útil para testar algoritmos de aprendizado de máquina em cenários controlados e conhecidos. Por exemplo, a função `make_classification` pode ser usada para gerar um conjunto de dados com classes bem definidas e distribuições controladas.\n",
    "\n",
    "Em resumo, o módulo `sklearn.datasets` fornece acesso a uma variedade de conjuntos de dados populares e ferramentas para gerar conjuntos de dados sintéticos, facilitando o processo de treinamento e teste de algoritmos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf0e44",
   "metadata": {},
   "source": [
    "o datasets é preprocessamento de dados?\n",
    "\n",
    "Não exatamente. O módulo `sklearn.datasets` fornece conjuntos de dados já pré-processados e prontos para serem usados em algoritmos de aprendizado de máquina, mas isso não significa que eles estejam completamente preparados para uso.\n",
    "\n",
    "O pré-processamento de dados é uma etapa importante no fluxo de trabalho de aprendizado de máquina, que envolve a limpeza, normalização, transformação e seleção de recursos dos dados. É importante garantir que os dados de entrada estejam em um formato adequado e que não contenham ruído ou informações redundantes que possam afetar a precisão e a eficácia dos algoritmos de aprendizado de máquina.\n",
    "\n",
    "Embora o módulo `sklearn.datasets` forneça conjuntos de dados que já foram pré-processados de uma forma geral, ainda é necessário realizar outras etapas de pré-processamento de dados antes de usá-los em um modelo de aprendizado de máquina específico. Essas etapas podem incluir a codificação de recursos categóricos, a normalização de dados numéricos e a seleção de recursos relevantes para o problema em questão.\n",
    "\n",
    "Em resumo, o módulo `sklearn.datasets` fornece um ponto de partida conveniente para o treinamento e teste de algoritmos de aprendizado de máquina, mas ainda é necessário realizar outras etapas de pré-processamento de dados para adaptar os conjuntos de dados às necessidades específicas de um modelo de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6defe7",
   "metadata": {},
   "source": [
    "então usa o datasets antes do aprendizado de maquina\n",
    "\n",
    "Sim, os conjuntos de dados fornecidos pelo módulo `sklearn.datasets` são geralmente usados antes de aplicar técnicas de aprendizado de máquina. Eles são usados principalmente para fins de treinamento e teste de algoritmos de aprendizado de máquina.\n",
    "\n",
    "Em geral, o processo de aprendizado de máquina envolve várias etapas, incluindo a aquisição e preparação dos dados, o treinamento e ajuste de um modelo de aprendizado de máquina nos dados, e a avaliação do desempenho do modelo em um conjunto de dados de teste. Os conjuntos de dados fornecidos pelo `sklearn.datasets` são geralmente usados na primeira etapa, para fornecer um conjunto de dados para serem usados no treinamento e teste de algoritmos de aprendizado de máquina.\n",
    "\n",
    "No entanto, é importante lembrar que o processo de preparação de dados é uma parte crítica do fluxo de trabalho de aprendizado de máquina e deve ser feito com cuidado e atenção aos detalhes. Os conjuntos de dados fornecidos pelo `sklearn.datasets` já passaram por algumas etapas de pré-processamento, mas ainda podem precisar de alguma limpeza e transformação antes de serem usados em um algoritmo de aprendizado de máquina.\n",
    "\n",
    "\n",
    "\n",
    "Os conjuntos de dados fornecidos pelo `sklearn.datasets` podem ser usados em uma ampla variedade de modelos de aprendizado de máquina. Alguns exemplos de modelos que podem ser usados incluem:\n",
    "\n",
    "- Regressão linear: utilizada para problemas de regressão, onde a saída é uma variável contínua.\n",
    "- Regressão logística: utilizada para problemas de classificação binária, onde a saída é uma variável categórica binária.\n",
    "- Árvores de decisão: utilizadas para problemas de classificação e regressão, onde os dados são divididos em subconjuntos com base em certos critérios de decisão.\n",
    "- Random forests: uma extensão das árvores de decisão que utiliza várias árvores para melhorar a precisão da predição.\n",
    "- Redes neurais: utilizadas para problemas de classificação e regressão, onde um grande número de variáveis de entrada pode ser considerado e modelado de forma não-linear.\n",
    "- Algoritmos de agrupamento: utilizados para agrupar dados sem rótulos em grupos ou clusters com base em sua semelhança.\n",
    "- Análise de componentes principais (PCA): utilizada para reduzir a dimensionalidade dos dados, preservando ao mesmo tempo o máximo de informação possível.\n",
    "\n",
    "Estes são apenas alguns exemplos de modelos de aprendizado de máquina que podem ser utilizados com os conjuntos de dados do `sklearn.datasets`. A escolha do modelo depende do tipo de problema que está sendo resolvido, bem como das características do conjunto de dados em questão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0593e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemplo\n",
    "\n",
    "# O sklearn possui alguns datasets que já estão disponíveis para uso, \n",
    "# como o conjunto de dados Iris, Boston Housing, entre outros. Para utilizar esses datasets, \n",
    "# basta importá-los do módulo sklearn.datasets. Por exemplo, para carregar o conjunto de dados Iris, \n",
    "# podemos fazer o seguinte:\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# O objeto iris é um dicionário-like com as seguintes chaves:\n",
    "\n",
    "# data: uma matriz numpy que contém as variáveis preditoras.\n",
    "# target: um vetor numpy que contém a variável resposta.\n",
    "# target_names: um vetor numpy com os nomes dos rótulos da variável resposta.\n",
    "# feature_names: um vetor numpy com os nomes das variáveis preditoras.\n",
    "# DESCR: uma descrição do conjunto de dados.\n",
    "# Podemos acessar cada uma dessas chaves para obter as informações relevantes. \n",
    "# Por exemplo, para obter os nomes das variáveis preditoras do conjunto de dados Iris, podemos fazer:\n",
    "\n",
    "print(iris.feature_names)\n",
    "\n",
    "# Para usar um conjunto de dados personalizado, você pode criar um arquivo .csv ou .\n",
    "# txt com os dados e importá-lo usando o pandas, em seguida, \n",
    "# converter os dados para numpy array usando o método values e, \n",
    "# em seguida, passar os dados para o modelo de aprendizado de máquina\n",
    "# usando as funções fornecidas pelo sklearn.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# carregar dados\n",
    "df = pd.read_csv('meu_dataset.csv')\n",
    "\n",
    "# dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.2)\n",
    "\n",
    "# treinar modelo\n",
    "# ...\n",
    "\n",
    "\n",
    "# outro exemplo\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# carrega o conjunto de dados Iris\n",
    "iris = load_iris()\n",
    "\n",
    "# divide o conjunto de dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "# treina um modelo de árvore de decisão\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# faz previsões usando o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calcula a acurácia do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {accuracy}\")\n",
    "\n",
    "# Este código carrega o conjunto de dados Iris usando load_iris(), divide os dados em conjuntos de treinamento e teste, \n",
    "# treina um modelo de árvore de decisão usando o conjunto de treinamento e, em seguida, faz previsões no conjunto de teste. \n",
    "# Finalmente, calcula a acurácia do modelo usando a métrica accuracy_score do sklearn.metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ff167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelos de datasets\n",
    "\n",
    "#sklearn.datasets: Conjuntos de dados \n",
    "#O sklearn.datasetsmódulo inclui utilitários para carregar conjuntos de dados, incluindo métodos para carregar e buscar conjuntos de dados de referência populares. Ele também possui alguns geradores de dados artificiais.\n",
    "\n",
    "\n",
    "# Carregadeiras  \n",
    "\n",
    "from sklearn.datasets import clear_data_home      # Exclua todo o conteúdo do cache inicial de dados.\n",
    "from sklearn.datasets import dump_svmlight_file   # Despeje o conjunto de dados no formato de arquivo svmlight / libsvm.\n",
    "from sklearn.datasets import fetch_20newsgroups   # Carregue os nomes de arquivos e dados do conjunto de dados de 20 grupos de notícias (classificação).\n",
    "from sklearn.datasets import fetch_20newsgroups_vectorized    # Carregue e vetorize o conjunto de dados de 20 grupos de notícias (classificação).\n",
    "from sklearn.datasets import fetch_california_housing         # Carregue o conjunto de dados habitacionais da Califórnia (regressão).\n",
    "from sklearn.datasets import fetch_covtype        # Carregue o conjunto de dados covertype (classificação).\n",
    "from sklearn.datasets import fetch_kddcup99       # Carregue o conjunto de dados kddcup99 (classificação).\n",
    "from sklearn.datasets import fetch_lfw_pairs      # Carregue o conjunto de dados de pares Labeled Faces in the Wild (LFW) (classificação).\n",
    "from sklearn.datasets import fetch_lfw_people     # Carregue o conjunto de dados de pessoas Labeled Faces in the Wild (LFW) (classificação).\n",
    "from sklearn.datasets import fetch_olivetti_faces # Carregue o conjunto de dados Olivetti faces da AT&T (classificação).\n",
    "from sklearn.datasets import fetch_openml         # Busque o conjunto de dados do openml por nome ou ID do conjunto de dados.\n",
    "from sklearn.datasets import fetch_rcv1           # Carregue o conjunto de dados multirrótulo RCV1 (classificação).\n",
    "from sklearn.datasets import fetch_species_distributions  # Carregador para conjunto de dados de distribuição de espécies de Phillips et.\n",
    "from sklearn.datasets import get_data_home                # Retorne o caminho do diretório de dados scikit-learn.\n",
    "from sklearn.datasets import load_breast_cancer           # Carregue e retorne o conjunto de dados de câncer de mama em Wisconsin (classificação).\n",
    "from sklearn.datasets import load_diabetes                # Carregue e retorne o conjunto de dados de diabetes (regressão).\n",
    "from sklearn.datasets import load_digits                  # Carregue e retorne o conjunto de dados de dígitos (classificação).\n",
    "from sklearn.datasets import load_files                   # Carregue arquivos de texto com categorias como nomes de subpastas.\n",
    "from sklearn.datasets import load_iris                    # Carregue e retorne o conjunto de dados da íris (classificação).\n",
    "from sklearn.datasets import load_linnerud                # Carregue e retorne o conjunto de dados Linnerud de exercício físico.\n",
    "from sklearn.datasets import load_sample_image            # Carregue a matriz numpy de uma única imagem de amostra.\n",
    "from sklearn.datasets import load_sample_images()         # Carregue imagens de amostra para manipulação de imagens.\n",
    "from sklearn.datasets import load_svmlight_file           # Carregue conjuntos de dados no formato svmlight / libsvm na matriz CSR esparsa.\n",
    "from sklearn.datasets import load_svmlight_files          # Carregue o conjunto de dados de vários arquivos no formato SVMlight.\n",
    "from sklearn.datasets import load_wine                    # Carregue e retorne o conjunto de dados do vinho (classificação).\n",
    "\n",
    "# Gerador de amostras \n",
    "\n",
    "from sklearn.datasets import make_biclusters          # Gere uma matriz de estrutura diagonal de bloco constante para biclustering.\n",
    "from sklearn.datasets import make_blobs               # Gere blobs gaussianos isotrópicos para agrupamento.\n",
    "from sklearn.datasets import make_checkerboard        # Gere uma matriz com estrutura quadriculada de blocos para biclustering.\n",
    "from sklearn.datasets import make_circles             # Faça um círculo grande contendo um círculo menor em 2d.\n",
    "from sklearn.datasets import make_classification      # Gere um problema de classificação aleatória de n classes.\n",
    "from sklearn.datasets import make_friedman1           # Gere o problema de regressão \"Friedman #1\".\n",
    "from sklearn.datasets import make_friedman2           # Gere o problema de regressão \"Friedman #2\".\n",
    "from sklearn.datasets import make_friedman3           # Gere o problema de regressão \"Friedman #3\".\n",
    "from sklearn.datasets import make_gaussian_quantiles  # Gere gaussianas isotrópicas e rotule amostras por quantil.\n",
    "from sklearn.datasets import make_hastie_10_2         # Gerar dados para classificação binária usada em Hastie et al. 2009, Exemplo 10.2.\n",
    "from sklearn.datasets import make_low_rank_matrix     # Gere uma matriz de classificação baixa com valores singulares em forma de sino.\n",
    "from sklearn.datasets import make_moons               # Faça dois semicírculos intercalados.\n",
    "from sklearn.datasets import make_multilabel_classification   # Gere um problema de classificação multirrótulo aleatório.\n",
    "from sklearn.datasets import make_regression                  # Gere um problema de regressão aleatória.\n",
    "from sklearn.datasets import make_s_curve                     # Gere um conjunto de dados de curva S.\n",
    "from sklearn.datasets import make_sparse_coded_signal         # Gere um sinal como uma combinação esparsa de elementos do dicionário.\n",
    "from sklearn.datasets import make_sparse_spd_matrix           # Gere uma matriz positiva definida simétrica esparsa.\n",
    "from sklearn.datasets import make_sparse_uncorrelated         # Gere um problema de regressão aleatória com projeto não correlacionado esparso.\n",
    "from sklearn.datasets import make_spd_matrix                  # Gere uma matriz aleatória simétrica positiva definida.\n",
    "from sklearn.datasets import make_swiss_roll                  # Gere um conjunto de dados de rocambole.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eecceac",
   "metadata": {},
   "source": [
    "O SMOTE (Synthetic Minority Over-sampling Technique) é uma técnica de sobreamostragem usada para lidar com desequilíbrios de classes em conjuntos de dados. Em muitos problemas de aprendizado de máquina, a classe minoritária (ou seja, a classe com menos exemplos) é de interesse particular e, muitas vezes, é difícil coletar mais exemplos dessa classe. Nesses casos, a técnica de sobreamostragem é usada para criar exemplos sintéticos da classe minoritária.\n",
    "\n",
    "O SMOTE é uma técnica que gera exemplos sintéticos interpolando entre exemplos existentes da classe minoritária. A ideia básica do SMOTE é gerar exemplos sintéticos para a classe minoritária, criando novos exemplos a partir dos exemplos existentes dessa classe. O SMOTE calcula a diferença entre cada exemplo da classe minoritária e seus vizinhos mais próximos e, em seguida, gera novos exemplos adicionando essa diferença a esses exemplos.\n",
    "\n",
    "O sklearn possui a implementação do SMOTE no módulo imblearn.over_sampling e pode ser usado da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemplo\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# gera um conjunto de dados sintético para fins de exemplo\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "                           weights=[0.1, 0.9], n_informative=3,\n",
    "                           n_redundant=1, flip_y=0, n_features=20,\n",
    "                           n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "\n",
    "# aplica o SMOTE para lidar com o desequilíbrio de classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd30bd7",
   "metadata": {},
   "source": [
    "Este código usa a função make_classification para gerar um conjunto de dados sintético com uma classe minoritária representando 10% dos exemplos. Em seguida, usa o SMOTE para gerar exemplos sintéticos e balancear as classes. Observe que fit_resample é usado para ajustar o modelo SMOTE aos dados e gerar novos exemplos.\n",
    "\n",
    "O SMOTE pode ser útil em muitas aplicações de aprendizado de máquina em que a classe minoritária é importante, como detecção de fraude, diagnóstico médico e muito mais.\n",
    "\n",
    "O SMOTE é uma técnica de sobreamostragem que está disponível no pacote imbalanced-learn (também conhecido como imblearn), que é uma biblioteca Python de amostragem para lidar com conjuntos de dados desequilibrados. Para instalar o imblearn, você pode usar o pip, assim como outras bibliotecas do Python.\n",
    "\n",
    "Para instalar o imblearn e, portanto, o SMOTE, execute o seguinte comando no seu terminal ou prompt de comando:\n",
    "\n",
    "pip install imbalanced-learn\n",
    "\n",
    "Após a instalação, você pode usar o SMOTE importando a classe SMOTE do módulo imblearn.over_sampling, como no exemplo que eu te mostrei anteriormente:\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129970f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188110fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sempre segue um padrão para aprendizado de maquina\n",
    "\n",
    "#1-passo\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#definir o x e y \n",
    "\n",
    "x = tabela['varias colunas']\n",
    "\n",
    "y = tabela['todas colunas menos target']\n",
    "\n",
    "#as colunas vão ter valores e o target é o bjetivo é a coluna de resposta \n",
    "\n",
    "#separando os dados para trainamento e teste da maquina predentiva\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x,y, test_size=0.2, random_state=42,stratify=y)\n",
    "\n",
    "\n",
    "# Treinando Regressão logistica     \n",
    "# Treinando Árvores de Decisão      \n",
    "# Treinando Máquina de Vetores de Suporte (SVM))\n",
    "\n",
    "\n",
    "\n",
    "#______________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "#importar a inteligencia artificial\n",
    "from sklearn.linear_model import LogisticRegression# treinar regressão logistica \n",
    "from sklearn.tree import DecisionTreeClassifier    # treinar arvore de decisão\n",
    "from sklearn.svm import SVC                        # treinar maquina de vetores de suporte\n",
    "\n",
    "\n",
    "# #criar a inteligencia artificial\n",
    "#aqui estou criando uma variavel e instanciando a função\n",
    "modelo_regressaologistica = LogisticRegression()# regressão logistica \n",
    "modelo_arvoredecisao = DecisionTreeClassifier() # arvore de decisão\n",
    "modelo_maquinavetoressuporte = SVC()            # maquina de vetores de suporte\n",
    "\n",
    "\n",
    "\n",
    "#treinar a inteligencia artificial\n",
    "modelo_regressaologistica.fit(x_treino,y_treino)    # regressão logistica\n",
    "modelo_arvoredecisao.fit(x_treino,y_treino)         # arvore de decisão\n",
    "modelo_maquinavetoressuporte.fit(x_treino,y_treino) # maquina de vetores de suporte\n",
    "\n",
    "\n",
    "\n",
    "previsao_regressaologistica = modelo_regressaologistica.predict(x_teste)       # regressão logistica\n",
    "previsa_arvoredecisao = modelo_arvoredecisao.predict(x_teste)                  # arvore de decisão\n",
    "previsao_maquinavetoressuporte = modelo_maquinavetoressuporte.predict(x_teste) # maquina de vetores de suporte\n",
    "\n",
    "\n",
    "#depois vou fazer os testes exite varios tipos de testes aqui vou citar alguns, \n",
    "#os testes são chamado de classificadores ou metricas\n",
    "\n",
    "\n",
    "\n",
    "#______________________________________________________________________________________________________________________________\n",
    "\n",
    "#teste com acuracia\n",
    "# previsao_regressaologistica=modelo_regressaologistica.predict(x_teste)\n",
    "# display(previsao_regressaologistica)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_regressaologistica = accuracy_score(y_teste, previsao_regressaologistica)       # regressão logistica\n",
    "accuracy_arvoredecisao = accuracy_score(y_teste,previsa_arvoredecisao)                   # arvore de decisão\n",
    "accuracy_maquinavetoressuporte = accuracy_score(y_teste, previsao_maquinavetoressuporte) # maquina de vetores de suporte\n",
    "\n",
    "# display(accuracy_maquinavetoressuporte)\n",
    "# print(\"Acurácia (Regressão Logística): {:.2f}%\".format(accuracy_regressaologistica * 100))\n",
    "# print(\"Acurácia (Árvores de Decisão): {:.2f}%\".format(accuracy_arvoredecisao * 100))\n",
    "# print(\"Acurácia (Máquina de Vetores de Suporte (SVM))): {:.2f}%\".format(accuracy_maquinavetoressuporte * 100))\n",
    "\n",
    "\n",
    "#teste com F1-score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "#observação existe o micro e o macro\n",
    "f1 = f1_score(y_teste, previsao_regressaologistica, average='micro')     # regressão logistica\n",
    "f2 = f1_score(y_teste, previsa_arvoredecisao, average='micro')           # arvore de decisão\n",
    "f3 = f1_score(y_teste, previsao_maquinavetoressuporte, average='micro')  # maquina de vetores de suporte\n",
    "\n",
    "print(\"f1_score (Regressão Logística): {:.2f}%\".format(f1* 100))\n",
    "print(\"f1_score (Árvores de Decisão): {:.2f}%\".format(f2 * 100))\n",
    "print(\"f1_score (Máquina de Vetores de Suporte (SVM))): {:.2f}%\".format(f3 * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65394227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6310ec26",
   "metadata": {},
   "source": [
    "Tipos de aprendizado de máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baafe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelos \n",
    "\n",
    "#sklearn.linear_model: Modelos Lineares\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# arvore de decisão\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier # Um classificador de árvore de decisão.\n",
    "from sklearn.tree import DecisionTreeRegressor  # Um regressor de árvore de decisão.\n",
    "from sklearn.tree import ExtraTreeClassifier    # Um classificador de árvore extremamente aleatório.\n",
    "from sklearn.tree import ExtraTreeRegressor     # Um regressor de árvore extremamente aleatório.\n",
    "\n",
    "\n",
    "# Máquinas de vetores de suporte \n",
    "\n",
    "from sklearn.svm import LinearSVC                 #Classificação de vetores de suporte linear.\n",
    "from sklearn.svm import LinearSVR                 #Regressão de vetor de suporte linear.\n",
    "from sklearn.svm import NuSVC                     #Classificação Nu-Support Vector.\n",
    "from sklearn.svm import NuSVR                     #Nu Support Vector Regressão.\n",
    "from sklearn.svm import OneClassSVM               #Detecção de outliers não supervisionados.\n",
    "from sklearn.svm import SVC                       #Classificação de vetores de suporte C.\n",
    "from sklearn.svm import SVR                       #Epsilon-Support Vector Regression.\n",
    "\n",
    "\n",
    "# Aprendizagem Semi-Supervisionada\n",
    "\n",
    "from sklearn.semi_supervised import LabelPropagation   # Classificador de propagação de rótulos.\n",
    "from sklearn.semi_supervised import LabelSpreading     # Modelo LabelSpreading para aprendizado semi-supervisionado.\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier     # Classificador de autotreinamento.\n",
    "\n",
    "# Modelos de redes neurais\n",
    "\n",
    "from sklearn.neural_network import BernoulliRBM  # Máquina Boltzmann Restrita de Bernoulli (RBM).\n",
    "from sklearn.neural_network import MLPClassifier # Classificador Perceptron multicamada.\n",
    "from sklearn.neural_network import MLPRegressor  # Regressor Perceptron multicamada.\n",
    "\n",
    "# Modelos Lineares \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression    # Classificador de regressão logística (também conhecido como logit, MaxEnt).\n",
    "from sklearn.linear_model import LogisticRegressionCV  # Classificador de CV de regressão logística (também conhecido como logit, MaxEnt).\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier # Classificador passivo-agressivo.\n",
    "from sklearn.linear_model import Perceptron                  # Classificador perceptron linear.\n",
    "from sklearn.linear_model import RidgeClassifier             # Classificador usando regressão de Ridge.\n",
    "from sklearn.linear_model import RidgeClassifierCV           # Classificador Ridge com validação cruzada integrada.\n",
    "from sklearn.linear_model import SGDClassifier               # Classificadores lineares (SVM, regressão logística, etc.) com treinamento SGD.SGDOneClassSVM# Resolve SVM linear de uma classe usando descida de gradiente estocástico.\n",
    "\n",
    "#  Regressores lineares clássicos \n",
    "\n",
    "from sklearn.linear_model import LinearRegression            # Regressão linear de mínimos quadrados ordinários.\n",
    "from sklearn.linear_model import Ridge                       # Mínimos quadrados lineares com regularização l2.\n",
    "from sklearn.linear_model import RidgeCV                     # Regressão Ridge com validação cruzada integrada.\n",
    "from sklearn.linear_model import SGDRegressor                # Modelo linear ajustado minimizando uma perda empírica regularizada com SGD.\n",
    "\n",
    "# Regressores com seleção de variável \n",
    "# Os estimadores a seguir têm procedimentos de ajuste de seleção de variável integrados, mas qualquer estimador que usa uma penalidade L1 ou rede elástica também executa seleção de variável: normalmente SGDRegressor ou SGDClassifiercom uma penalidade apropriada.\n",
    "\n",
    "from sklearn.linear_model import ElasticNet               # Regressão linear com prioris L1 e L2 combinados como regularizador.\n",
    "from sklearn.linear_model import ElasticNetCV             # Modelo Elastic Net com ajuste iterativo ao longo de um caminho de regularização.\n",
    "from sklearn.linear_model import Lars                     # Modelo de Regressão de Ângulo Mínimo, também conhecido como\n",
    "from sklearn.linear_model import LarsCV                   # Modelo de Regressão de Ângulo Mínimo com validação cruzada.\n",
    "from sklearn.linear_model import Lasso                    # Modelo Linear treinado com L1 antes como regularizador (também conhecido como Lasso).\n",
    "from sklearn.linear_model import LassoCV                  # Modelo linear de laço com ajuste iterativo ao longo de um caminho de regularização.\n",
    "from sklearn.linear_model import LassoLars                # Modelo de laço ajustado com Regressão de Ângulo Mínimo, também conhecido como\n",
    "from sklearn.linear_model import LassoLarsCV              # Lasso com validação cruzada, usando o algoritmo LARS.\n",
    "from sklearn.linear_model import LassoLarsIC              # Modelo de laço ajustado com Lars usando BIC ou AIC para seleção de modelo.\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit   # Modelo Orthogonal Matching Pursuit (OMP).\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuitCV # Modelo de busca de correspondência ortogonal (OMP) com validação cruzada.\n",
    "\n",
    "# regressores bayesianos \n",
    "from sklearn.linear_model import ARDRegression               # Regressão ARD bayesiana.\n",
    "from sklearn.linear_model import BayesianRidge               # Regressão da crista bayesiana.\n",
    "\n",
    "# Regressores lineares multitarefa com seleção de variável \n",
    "# Esses estimadores ajustam vários problemas de regressão (ou tarefas) em conjunto, enquanto induzem coeficientes esparsos. Embora os coeficientes inferidos possam diferir entre as tarefas, eles são obrigados a concordar com os recursos selecionados (coeficientes diferentes de zero).\n",
    "\n",
    "from sklearn.linear_model import MultiTaskElasticNet          # Modelo ElasticNet multitarefa treinado com norma mista L1/L2 como regularizador.\n",
    "from sklearn.linear_model import MultiTaskElasticNetCV        # Multitarefa L1/L2 ElasticNet com validação cruzada integrada.\n",
    "from sklearn.linear_model import MultiTaskLasso               # Modelo de laço multitarefa treinado com norma mista L1/L2 como regularizador.\n",
    "from sklearn.linear_model import MultiTaskLassoCV             # Modelo de laço multitarefa treinado com norma mista L1/L2 como regularizador.\n",
    "\n",
    "# Regressores robustos a outliers \n",
    "# Qualquer estimador usando a perda de Huber também seria robusto para outliers, por exemplo, SGDRegressorcom loss='huber'.\n",
    "\n",
    "from sklearn.linear_model import HuberRegressor           # Modelo de regressão linear L2 regularizado que é robusto a outliers.\n",
    "from sklearn.linear_model import QuantileRegressor        # Modelo de regressão linear que prevê quantis condicionais.\n",
    "from sklearn.linear_model import RANSACRegressor          # Algoritmo RANSAC (RANdom SAmple Consensus).\n",
    "from sklearn.linear_model import TheilSenRegressor        # Theil-Sen Estimator: modelo robusto de regressão multivariada.\n",
    "    \n",
    "# Modelos lineares generalizados (GLM) para regressão \n",
    "# Esses modelos permitem que as variáveis de resposta tenham distribuições de erro diferentes de uma distribuição normal:\n",
    "\n",
    "from sklearn.linear_model import PoissonRegressor       # Modelo Linear Generalizado com distribuição de Poisson. \n",
    "from sklearn.linear_model import TweedieRegressor       # Modelo Linear Generalizado com uma distribuição Tweedie.\n",
    "from sklearn.linear_model import ammaRegressor          # Modelo Linear Generalizado com distribuição Gama.\n",
    "\n",
    "# Diversos \n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor       # Passivo Agressivo Regressor.\n",
    "from sklearn.linear_model import enet_path                        # Calcule o caminho líquido elástico com descida coordenada.\n",
    "from sklearn.linear_model import lars_path                        # Calcule a regressão do ângulo mínimo ou o caminho do laço usando o algoritmo LARS [1].\n",
    "from sklearn.linear_model import lars_path_gram                   # O lars_path no modo de estatísticas suficientes [1].\n",
    "from sklearn.linear_model import lasso_path                       # Calcule o caminho Lasso com descida coordenada.\n",
    "from sklearn.linear_model import orthogonal_mp                    # Perseguição de Correspondência Ortogonal (OMP).\n",
    "from sklearn.linear_model import orthogonal_mp_gram               # Gram Orthogonal Matching Pursuit (OMP).\n",
    "from sklearn.linear_model import ridge_regression                 # Resolva a equação do cume pelo método das equações normais.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
